{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "x = tf.placeholder(tf.float32, shape=[batch_size, N])\n",
    "y = tf.placeholder(tf.float32, shape=[batch_size, K])\n",
    "\n",
    "net_dict = {}\n",
    "bp_iter_num = 5\n",
    "RNN = 1\n",
    "\n",
    "inf_num = 1000\n",
    "    \n",
    "# initial\n",
    "for i in range(n+1):\n",
    "    for j in range(N):\n",
    "        net_dict[\"L_{0}{1}{2}\".format(i,j,0)] = tf.zeros((batch_size))\n",
    "        net_dict[\"R_{0}{1}{2}\".format(i,j,0)] = tf.zeros((batch_size))\n",
    "\n",
    "if(RNN):\n",
    "    LV = tf.Variable(np.float32(np.ones((n,N,1))))\n",
    "    RV = tf.Variable(np.float32(np.ones((n,N,1))))\n",
    "else:\n",
    "    LV = tf.Variable(np.float32(np.ones((n,N,bp_iter_num))))\n",
    "    RV = tf.Variable(np.float32(np.ones((n,N,bp_iter_num))))\n",
    "\n",
    "for j in range(N):\n",
    "    net_dict[\"L_{0}{1}{2}\".format(n,j,0)] = tf.ones((1))*x[:,j]    \n",
    "    if(FZlookup[j] == 0):\n",
    "        net_dict[\"R_{0}{1}{2}\".format(0,j,0)] = tf.ones((batch_size))*inf_num\n",
    "\n",
    "loss = 0\n",
    "\n",
    "# bp algorithm\n",
    "for k in range(bp_iter_num):\n",
    "    if(RNN):\n",
    "        itr = 0\n",
    "    else:\n",
    "        itr = k\n",
    "    for i in range(n,0,-1):\n",
    "        for phi in range(2**i):\n",
    "            psi = int(np.floor(phi/2))\n",
    "            if(np.mod(phi,2)!=0):\n",
    "                for omega in range(2**(n-i)):\n",
    "                    net_dict[\"R_{0}{1}{2}\".format(n+1-i,psi+2*omega*2**(i-1),0)] = RV[n-i,psi+2*omega*2**(i-1),itr]*fFunction(net_dict[\"L_{0}{1}{2}\".format(n+1-i,psi+(2*omega+1)*2**(i-1),0)]+net_dict[\"R_{0}{1}{2}\".format(n-i,phi+omega*2**i,0)], net_dict[\"R_{0}{1}{2}\".format(n-i,phi-1+omega*2**i,0)])\n",
    "                    net_dict[\"R_{0}{1}{2}\".format(n+1-i,psi+(2*omega+1)*2**(i-1),0)] = net_dict[\"R_{0}{1}{2}\".format(n-i,phi+omega*2**i,0)]+RV[n-i,psi+(2*omega+1)*2**(i-1),itr]*fFunction(net_dict[\"L_{0}{1}{2}\".format(n+1-i,psi+(2*omega)*2**(i-1),0)],net_dict[\"R_{0}{1}{2}\".format(n-i,phi-1+omega*2**i,0)])\n",
    "    for i in range(1,n+1):\n",
    "        for phi in range(2**i):\n",
    "            psi = int(np.floor(phi/2))\n",
    "            for omega in range(2**(n-i)):\n",
    "                if(np.mod(phi,2)==0):\n",
    "                    net_dict[\"L_{0}{1}{2}\".format(n-i,phi+omega*2**i,0)] = LV[n-i,phi+omega*2**i,itr]*fFunction(net_dict[\"L_{0}{1}{2}\".format(n+1-i,psi+2*omega*2**(i-1),0)],net_dict[\"L_{0}{1}{2}\".format(n+1-i,psi+(2*omega+1)*2**(i-1),0)]+net_dict[\"R_{0}{1}{2}\".format(n-i,phi+1+omega*2**i,0)])             \n",
    "                else:\n",
    "                    net_dict[\"L_{0}{1}{2}\".format(n-i,phi+omega*2**i,0)] = net_dict[\"L_{0}{1}{2}\".format(n+1-i,psi+(2*omega+1)*2**(i-1),0)]+LV[n-i,phi+omega*2**i,itr]*fFunction(net_dict[\"L_{0}{1}{2}\".format(n+1-i,psi+2*omega*2**(i-1),0)],net_dict[\"R_{0}{1}{2}\".format(n-i,phi-1+omega*2**i,0)])\n",
    "\n",
    "y_output = tf.zeros((1))\n",
    "for i in range(N):\n",
    "    if(FZlookup[i] == -1):\n",
    "        y_output = tf.concat([y_output,net_dict[\"L_{0}{1}{2}\".format(0,i,0)]],0)\n",
    "y_output = tf.transpose(tf.reshape(y_output[1:],(K,batch_size)))*-1\n",
    "loss = loss + 1.0*tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=y_output,labels=y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_ber = 1\n",
    "load_weight = 0\n",
    "quantize_weight = 0 #0 for non-quantize, 1 for normal, 2 for binarized, 3 for bin, 4 for binarized bin\n",
    "bin_bit = 4  # number of different value\n",
    "binary_prec = 4 # binary precision, binary_prec must >= bin_bit\n",
    "\n",
    "if not (load_weight):\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(loss)\n",
    "sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))  #allow tensorflow to automatically allocate device\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "if(quantize_weight == 0):\n",
    "    weight_path = '{0}_{1}_{2}_{3}_{4}_{5}.ckpt'.format(N,ebn0[0],ebn0[-1],bp_iter_num,RNN,quantize_weight)\n",
    "elif(quantize_weight == 1 or quantize_weight == 2):\n",
    "    weight_path = '{0}_{1}_{2}_{3}_{4}_{5}_{6}.ckpt'.format(N,ebn0[0],ebn0[-1],bp_iter_num,RNN,quantize_weight,binary_prec)\n",
    "elif(quantize_weight == 3):\n",
    "    weight_path = '{0}_{1}_{2}_{3}_{4}_{5}_{6}.ckpt'.format(N,ebn0[0],ebn0[-1],bp_iter_num,RNN,quantize_weight,bin_bit)\n",
    "else:\n",
    "    weight_path = '{0}_{1}_{2}_{3}_{4}_{5}_{6}_{7}.ckpt'.format(N,ebn0[0],ebn0[-1],bp_iter_num,RNN,quantize_weight,bin_bit,binary_prec)    \n",
    "    \n",
    "if not(load_weight):\n",
    "    for itr in range(50):\n",
    "        train_nfails = 0\n",
    "        train_loss = 0\n",
    "        train_nframe = 0\n",
    "        val_nfails = 0\n",
    "        val_loss = 0\n",
    "        # train\n",
    "        wordRandom = np.random.RandomState(word_seed)\n",
    "        noiseRandom = np.random.RandomState(noise_seed)        \n",
    "        for i in range(batches_train):\n",
    "            x_train, y_train = gendata(0,False,False)\n",
    "            y_pred, _loss, _ = sess.run(fetches=[y_output, loss, train_step], feed_dict={x: x_train, y: y_train})\n",
    "            train_loss = _loss + train_loss\n",
    "            uhat = np.zeros((batch_size,K))\n",
    "            uhat[y_pred>=0] = 1\n",
    "            train_nfails = train_nfails + sum(sum(uhat!=y_train))\n",
    "            train_nframe = train_nframe + sum(np.sum(uhat!=y_train,1)!=0)\n",
    "        # quantize\n",
    "        if(quantize_weight):\n",
    "            LWeight, RWeight = get_weight(sess, LV, RV)\n",
    "            if(quantize_weight == 1):\n",
    "                LWeight = quantize(LWeight,binary_prec)\n",
    "                RWeight = quantize(RWeight,binary_prec)\n",
    "            elif(quantize_weight == 2):\n",
    "                LWeight = quantizeToClosestBinary(LWeight,binary_prec)\n",
    "                RWeight = quantizeToClosestBinary(RWeight,binary_prec)\n",
    "            elif(quantize_weight == 3):\n",
    "                LWeight = quantizeToBins(LWeight,bin_bit)\n",
    "                RWeight = quantizeToBins(RWeight,bin_bit)\n",
    "            else:\n",
    "                LWeight = BINARYquantizeToBins(LWeight,bin_bit,binary_prec)\n",
    "                RWeight = BINARYquantizeToBins(RWeight,bin_bit,binary_prec)            \n",
    "            assign_weight(LWeight, RWeight, sess, LV, RV)\n",
    "        # validation\n",
    "        for i in range(batches_val):\n",
    "            x_val, y_val = gendata(0,False,False)\n",
    "            y_pred, _loss = sess.run(fetches=[y_output, loss], feed_dict={x: x_val, y: y_val})\n",
    "            val_loss = _loss + val_loss\n",
    "            uhat = np.zeros((batch_size,K))\n",
    "            uhat[y_pred>=0] = 1\n",
    "            val_nfails = val_nfails + sum(sum(uhat!=y_val))\n",
    "        if(best_val_ber > (val_nfails/(batch_size*K*batches_val))):\n",
    "            best_val_ber = val_nfails/(batch_size*K*batches_val)\n",
    "            saver.save(sess, './weight.ckpt')\n",
    "            Nobetter = 0\n",
    "        else:\n",
    "            Nobetter = Nobetter + 1\n",
    "        if(Nobetter > patience):\n",
    "            break    \n",
    "        print('Epoch:\\t{0:d}'.format(itr),'\\tLoss:{0:.7f}'.format(train_loss/batches_train),'\\tBER:{0:.7f}'.format(train_nfails/(batch_size*K*batches_train)),'\\tFER:{0:.7f}'.format(train_nframe/(batch_size*batches_train)),'\\tVal Loss:{0:.7f}'.format(val_loss/batches_val),'\\tVal BER:{0:.7f}'.format(val_nfails/(batch_size*K*batches_val)))\n",
    "\n",
    "# test\n",
    "if(load_weight):\n",
    "    saver.restore(sess, 'Weight/'+weight_path)\n",
    "else:\n",
    "    saver.restore(sess, './weight.ckpt')\n",
    "\n",
    "test_nfails = np.zeros((len(ebn0)))\n",
    "test_loss = np.zeros((len(ebn0)))\n",
    "test_nframe = np.zeros((len(ebn0)))\n",
    "wordRandom = np.random.RandomState(word_seed-200)\n",
    "noiseRandom = np.random.RandomState(noise_seed-200) \n",
    "for j in range(len(ebn0)):\n",
    "    for i in range(int(batches_test/len(ebn0))):\n",
    "        x_test, y_test = gendata(j,True,False)\n",
    "        y_pred, _loss = sess.run(fetches=[y_output, loss], feed_dict={x: x_test, y: y_test})\n",
    "        test_loss[j] = _loss + test_loss[j]\n",
    "        uhat = np.zeros((batch_size,K))\n",
    "        uhat[y_pred>=0] = 1\n",
    "        test_nfails[j] = test_nfails[j] + sum(sum(uhat!=y_test))\n",
    "        test_nframe[j] = test_nframe[j] + sum(np.sum(uhat!=y_test,1)!=0)\n",
    "print('Test SNR:  ',ebn0)\n",
    "print('Test Loss: ',test_loss/int(batches_test/len(ebn0)))\n",
    "print('Test BER:  ',test_nfails/(batch_size*K*(batches_test/len(ebn0))))\n",
    "print('Test FER:  ',test_nframe/(batch_size*(batches_test/len(ebn0))))\n",
    "if not(load_weight):\n",
    "    saver.save(sess, 'Weight/'+weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_results = open('Results/'+weight_path[:-4]+'txt', 'w+')\n",
    "f_results.write('Test SNR:    ' + str(ebn0) + '\\n')\n",
    "f_results.write('Test Loss:   ' + str(test_loss/int(batches_test/len(ebn0))) + '\\n')\n",
    "f_results.write('Test BER:    ' + str(test_nfails/(batch_size*K*(batches_test/len(ebn0)))) + '\\n')\n",
    "f_results.write('Test FER:    ' + str(test_nframe/(batch_size*(batches_test/len(ebn0)))) + '\\n')\n",
    "f_results.write('NumOfWord:   ' + str(numOfWord) + '\\n')\n",
    "f_results.write('Batch_size:  ' + str(batch_size) + '\\n')\n",
    "f_results.write('Batch_train: ' + str(batches_train) + '\\n')\n",
    "f_results.write('Batch_test:  ' + str(batches_test) + '\\n')\n",
    "f_results.write('Batch_val:   ' + str(batches_val) + '\\n')\n",
    "f_results.flush()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
